{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASI assessed exercice\n",
    "\n",
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv',delimiter=\",\")[:5558].drop([\"ID_code\",\"target\"], axis = 1)\n",
    "train_labels = pd.read_csv('data/train.csv',delimiter=\",\")[:5558][\"target\"]\n",
    "test_data = pd.read_csv('data/train.csv',delimiter=\",\")[5558:].drop([\"ID_code\",\"target\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095718</td>\n",
       "      <td>10.697905</td>\n",
       "      <td>-1.666913</td>\n",
       "      <td>10.675315</td>\n",
       "      <td>6.817630</td>\n",
       "      <td>11.089890</td>\n",
       "      <td>-5.114018</td>\n",
       "      <td>5.385003</td>\n",
       "      <td>16.603072</td>\n",
       "      <td>0.324868</td>\n",
       "      <td>...</td>\n",
       "      <td>3.210568</td>\n",
       "      <td>7.392127</td>\n",
       "      <td>1.963794</td>\n",
       "      <td>3.328075</td>\n",
       "      <td>18.011745</td>\n",
       "      <td>-0.178757</td>\n",
       "      <td>2.258062</td>\n",
       "      <td>8.906045</td>\n",
       "      <td>15.897304</td>\n",
       "      <td>-3.465982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294230</td>\n",
       "      <td>3.034355</td>\n",
       "      <td>4.097794</td>\n",
       "      <td>2.636887</td>\n",
       "      <td>2.059984</td>\n",
       "      <td>1.618655</td>\n",
       "      <td>7.876235</td>\n",
       "      <td>0.864426</td>\n",
       "      <td>3.414093</td>\n",
       "      <td>3.339810</td>\n",
       "      <td>...</td>\n",
       "      <td>4.598166</td>\n",
       "      <td>3.019488</td>\n",
       "      <td>1.479331</td>\n",
       "      <td>3.991366</td>\n",
       "      <td>3.191170</td>\n",
       "      <td>1.413091</td>\n",
       "      <td>5.471104</td>\n",
       "      <td>0.929138</td>\n",
       "      <td>2.989720</td>\n",
       "      <td>10.523237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.335000</td>\n",
       "      <td>-13.422700</td>\n",
       "      <td>3.678500</td>\n",
       "      <td>1.154100</td>\n",
       "      <td>5.943000</td>\n",
       "      <td>-29.013300</td>\n",
       "      <td>2.673300</td>\n",
       "      <td>8.017000</td>\n",
       "      <td>-9.476600</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.593900</td>\n",
       "      <td>-1.215800</td>\n",
       "      <td>-3.102800</td>\n",
       "      <td>-9.442700</td>\n",
       "      <td>10.767500</td>\n",
       "      <td>-4.488300</td>\n",
       "      <td>-12.325500</td>\n",
       "      <td>6.256900</td>\n",
       "      <td>7.233700</td>\n",
       "      <td>-36.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.504500</td>\n",
       "      <td>-4.772600</td>\n",
       "      <td>8.685550</td>\n",
       "      <td>5.245550</td>\n",
       "      <td>9.879200</td>\n",
       "      <td>-11.342550</td>\n",
       "      <td>4.748200</td>\n",
       "      <td>14.050825</td>\n",
       "      <td>-2.310150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176300</td>\n",
       "      <td>5.131275</td>\n",
       "      <td>0.932875</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>15.556550</td>\n",
       "      <td>-1.196825</td>\n",
       "      <td>-2.025725</td>\n",
       "      <td>8.246700</td>\n",
       "      <td>13.859600</td>\n",
       "      <td>-11.547400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.538050</td>\n",
       "      <td>-1.652950</td>\n",
       "      <td>10.518200</td>\n",
       "      <td>6.846150</td>\n",
       "      <td>11.111500</td>\n",
       "      <td>-4.979900</td>\n",
       "      <td>5.351900</td>\n",
       "      <td>16.576850</td>\n",
       "      <td>0.368400</td>\n",
       "      <td>...</td>\n",
       "      <td>3.215100</td>\n",
       "      <td>7.286250</td>\n",
       "      <td>1.975200</td>\n",
       "      <td>3.391750</td>\n",
       "      <td>17.958750</td>\n",
       "      <td>-0.224350</td>\n",
       "      <td>2.466350</td>\n",
       "      <td>8.898200</td>\n",
       "      <td>15.954500</td>\n",
       "      <td>-3.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.757575</td>\n",
       "      <td>1.285050</td>\n",
       "      <td>12.467150</td>\n",
       "      <td>8.364150</td>\n",
       "      <td>12.275175</td>\n",
       "      <td>0.962025</td>\n",
       "      <td>5.982350</td>\n",
       "      <td>19.125825</td>\n",
       "      <td>2.976075</td>\n",
       "      <td>...</td>\n",
       "      <td>6.411775</td>\n",
       "      <td>9.491400</td>\n",
       "      <td>2.981000</td>\n",
       "      <td>6.168375</td>\n",
       "      <td>20.510225</td>\n",
       "      <td>0.764450</td>\n",
       "      <td>6.485800</td>\n",
       "      <td>9.597975</td>\n",
       "      <td>18.102075</td>\n",
       "      <td>4.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.289300</td>\n",
       "      <td>8.443100</td>\n",
       "      <td>18.347700</td>\n",
       "      <td>12.906600</td>\n",
       "      <td>15.193600</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>7.875500</td>\n",
       "      <td>26.284800</td>\n",
       "      <td>9.103800</td>\n",
       "      <td>...</td>\n",
       "      <td>16.614400</td>\n",
       "      <td>16.398500</td>\n",
       "      <td>7.139000</td>\n",
       "      <td>16.103300</td>\n",
       "      <td>26.628500</td>\n",
       "      <td>3.555400</td>\n",
       "      <td>14.843600</td>\n",
       "      <td>11.843900</td>\n",
       "      <td>25.442200</td>\n",
       "      <td>25.094400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target        var_0        var_1        var_2        var_3  \\\n",
       "count  5558.000000  5558.000000  5558.000000  5558.000000  5558.000000   \n",
       "mean      0.095718    10.697905    -1.666913    10.675315     6.817630   \n",
       "std       0.294230     3.034355     4.097794     2.636887     2.059984   \n",
       "min       0.000000     1.335000   -13.422700     3.678500     1.154100   \n",
       "25%       0.000000     8.504500    -4.772600     8.685550     5.245550   \n",
       "50%       0.000000    10.538050    -1.652950    10.518200     6.846150   \n",
       "75%       0.000000    12.757575     1.285050    12.467150     8.364150   \n",
       "max       1.000000    19.289300     8.443100    18.347700    12.906600   \n",
       "\n",
       "             var_4        var_5        var_6        var_7        var_8  ...  \\\n",
       "count  5558.000000  5558.000000  5558.000000  5558.000000  5558.000000  ...   \n",
       "mean     11.089890    -5.114018     5.385003    16.603072     0.324868  ...   \n",
       "std       1.618655     7.876235     0.864426     3.414093     3.339810  ...   \n",
       "min       5.943000   -29.013300     2.673300     8.017000    -9.476600  ...   \n",
       "25%       9.879200   -11.342550     4.748200    14.050825    -2.310150  ...   \n",
       "50%      11.111500    -4.979900     5.351900    16.576850     0.368400  ...   \n",
       "75%      12.275175     0.962025     5.982350    19.125825     2.976075  ...   \n",
       "max      15.193600    17.251600     7.875500    26.284800     9.103800  ...   \n",
       "\n",
       "           var_190      var_191      var_192      var_193      var_194  \\\n",
       "count  5558.000000  5558.000000  5558.000000  5558.000000  5558.000000   \n",
       "mean      3.210568     7.392127     1.963794     3.328075    18.011745   \n",
       "std       4.598166     3.019488     1.479331     3.991366     3.191170   \n",
       "min     -11.593900    -1.215800    -3.102800    -9.442700    10.767500   \n",
       "25%      -0.176300     5.131275     0.932875     0.587500    15.556550   \n",
       "50%       3.215100     7.286250     1.975200     3.391750    17.958750   \n",
       "75%       6.411775     9.491400     2.981000     6.168375    20.510225   \n",
       "max      16.614400    16.398500     7.139000    16.103300    26.628500   \n",
       "\n",
       "           var_195      var_196      var_197      var_198      var_199  \n",
       "count  5558.000000  5558.000000  5558.000000  5558.000000  5558.000000  \n",
       "mean     -0.178757     2.258062     8.906045    15.897304    -3.465982  \n",
       "std       1.413091     5.471104     0.929138     2.989720    10.523237  \n",
       "min      -4.488300   -12.325500     6.256900     7.233700   -36.302500  \n",
       "25%      -1.196825    -2.025725     8.246700    13.859600   -11.547400  \n",
       "50%      -0.224350     2.466350     8.898200    15.954500    -3.015200  \n",
       "75%       0.764450     6.485800     9.597975    18.102075     4.706500  \n",
       "max       3.555400    14.843600    11.843900    25.442200    25.094400  \n",
       "\n",
       "[8 rows x 201 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>5558.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "      <td>5557.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.648842</td>\n",
       "      <td>-1.586676</td>\n",
       "      <td>10.743441</td>\n",
       "      <td>6.771747</td>\n",
       "      <td>11.068790</td>\n",
       "      <td>-4.982362</td>\n",
       "      <td>5.415405</td>\n",
       "      <td>16.617403</td>\n",
       "      <td>0.246438</td>\n",
       "      <td>7.556011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.265992</td>\n",
       "      <td>7.503558</td>\n",
       "      <td>1.921922</td>\n",
       "      <td>3.324059</td>\n",
       "      <td>18.046049</td>\n",
       "      <td>-0.166249</td>\n",
       "      <td>2.285127</td>\n",
       "      <td>8.918706</td>\n",
       "      <td>15.870934</td>\n",
       "      <td>-3.410438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.052894</td>\n",
       "      <td>4.050896</td>\n",
       "      <td>2.632958</td>\n",
       "      <td>2.045038</td>\n",
       "      <td>1.624257</td>\n",
       "      <td>7.770581</td>\n",
       "      <td>0.874270</td>\n",
       "      <td>3.448866</td>\n",
       "      <td>3.269242</td>\n",
       "      <td>1.235005</td>\n",
       "      <td>...</td>\n",
       "      <td>4.559551</td>\n",
       "      <td>3.067594</td>\n",
       "      <td>1.471492</td>\n",
       "      <td>4.006826</td>\n",
       "      <td>3.167623</td>\n",
       "      <td>1.421603</td>\n",
       "      <td>5.404760</td>\n",
       "      <td>0.907914</td>\n",
       "      <td>3.029635</td>\n",
       "      <td>10.357434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.597900</td>\n",
       "      <td>-12.163700</td>\n",
       "      <td>3.309000</td>\n",
       "      <td>0.937700</td>\n",
       "      <td>6.031500</td>\n",
       "      <td>-24.330000</td>\n",
       "      <td>2.829600</td>\n",
       "      <td>7.642200</td>\n",
       "      <td>-9.991100</td>\n",
       "      <td>4.364400</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.730400</td>\n",
       "      <td>-1.504300</td>\n",
       "      <td>-3.515900</td>\n",
       "      <td>-8.693000</td>\n",
       "      <td>10.387000</td>\n",
       "      <td>-4.297000</td>\n",
       "      <td>-11.882800</td>\n",
       "      <td>6.322700</td>\n",
       "      <td>7.417100</td>\n",
       "      <td>-36.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.421625</td>\n",
       "      <td>-4.777975</td>\n",
       "      <td>8.774000</td>\n",
       "      <td>5.252400</td>\n",
       "      <td>9.866450</td>\n",
       "      <td>-11.048425</td>\n",
       "      <td>4.772475</td>\n",
       "      <td>13.923100</td>\n",
       "      <td>-2.226450</td>\n",
       "      <td>6.597875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017500</td>\n",
       "      <td>5.156500</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>15.675300</td>\n",
       "      <td>-1.209700</td>\n",
       "      <td>-1.939200</td>\n",
       "      <td>8.270300</td>\n",
       "      <td>13.869000</td>\n",
       "      <td>-11.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.488600</td>\n",
       "      <td>-1.530850</td>\n",
       "      <td>10.562600</td>\n",
       "      <td>6.774000</td>\n",
       "      <td>11.086500</td>\n",
       "      <td>-4.732750</td>\n",
       "      <td>5.411050</td>\n",
       "      <td>16.559450</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>7.614050</td>\n",
       "      <td>...</td>\n",
       "      <td>3.314400</td>\n",
       "      <td>7.385800</td>\n",
       "      <td>1.880500</td>\n",
       "      <td>3.397700</td>\n",
       "      <td>17.951200</td>\n",
       "      <td>-0.193500</td>\n",
       "      <td>2.416300</td>\n",
       "      <td>8.893300</td>\n",
       "      <td>15.908100</td>\n",
       "      <td>-2.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.759825</td>\n",
       "      <td>1.391200</td>\n",
       "      <td>12.534550</td>\n",
       "      <td>8.296000</td>\n",
       "      <td>12.250225</td>\n",
       "      <td>0.983925</td>\n",
       "      <td>6.004675</td>\n",
       "      <td>19.210225</td>\n",
       "      <td>2.851975</td>\n",
       "      <td>8.576125</td>\n",
       "      <td>...</td>\n",
       "      <td>6.437200</td>\n",
       "      <td>9.608600</td>\n",
       "      <td>2.942900</td>\n",
       "      <td>6.212400</td>\n",
       "      <td>20.501100</td>\n",
       "      <td>0.799800</td>\n",
       "      <td>6.485300</td>\n",
       "      <td>9.601100</td>\n",
       "      <td>18.088800</td>\n",
       "      <td>4.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.092700</td>\n",
       "      <td>8.584900</td>\n",
       "      <td>18.157600</td>\n",
       "      <td>12.977300</td>\n",
       "      <td>15.366400</td>\n",
       "      <td>15.557100</td>\n",
       "      <td>7.788900</td>\n",
       "      <td>26.965600</td>\n",
       "      <td>8.665300</td>\n",
       "      <td>10.414600</td>\n",
       "      <td>...</td>\n",
       "      <td>16.046800</td>\n",
       "      <td>16.159200</td>\n",
       "      <td>7.051500</td>\n",
       "      <td>16.782600</td>\n",
       "      <td>27.485800</td>\n",
       "      <td>3.534500</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>11.722300</td>\n",
       "      <td>24.639400</td>\n",
       "      <td>25.831600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             var_0        var_1        var_2        var_3        var_4  \\\n",
       "count  5558.000000  5558.000000  5558.000000  5558.000000  5558.000000   \n",
       "mean     10.648842    -1.586676    10.743441     6.771747    11.068790   \n",
       "std       3.052894     4.050896     2.632958     2.045038     1.624257   \n",
       "min       0.597900   -12.163700     3.309000     0.937700     6.031500   \n",
       "25%       8.421625    -4.777975     8.774000     5.252400     9.866450   \n",
       "50%      10.488600    -1.530850    10.562600     6.774000    11.086500   \n",
       "75%      12.759825     1.391200    12.534550     8.296000    12.250225   \n",
       "max      19.092700     8.584900    18.157600    12.977300    15.366400   \n",
       "\n",
       "             var_5        var_6        var_7        var_8        var_9  ...  \\\n",
       "count  5558.000000  5558.000000  5558.000000  5558.000000  5558.000000  ...   \n",
       "mean     -4.982362     5.415405    16.617403     0.246438     7.556011  ...   \n",
       "std       7.770581     0.874270     3.448866     3.269242     1.235005  ...   \n",
       "min     -24.330000     2.829600     7.642200    -9.991100     4.364400  ...   \n",
       "25%     -11.048425     4.772475    13.923100    -2.226450     6.597875  ...   \n",
       "50%      -4.732750     5.411050    16.559450     0.268200     7.614050  ...   \n",
       "75%       0.983925     6.004675    19.210225     2.851975     8.576125  ...   \n",
       "max      15.557100     7.788900    26.965600     8.665300    10.414600  ...   \n",
       "\n",
       "           var_190      var_191      var_192      var_193      var_194  \\\n",
       "count  5557.000000  5557.000000  5557.000000  5557.000000  5557.000000   \n",
       "mean      3.265992     7.503558     1.921922     3.324059    18.046049   \n",
       "std       4.559551     3.067594     1.471492     4.006826     3.167623   \n",
       "min     -10.730400    -1.504300    -3.515900    -8.693000    10.387000   \n",
       "25%      -0.017500     5.156500     0.891100     0.566900    15.675300   \n",
       "50%       3.314400     7.385800     1.880500     3.397700    17.951200   \n",
       "75%       6.437200     9.608600     2.942900     6.212400    20.501100   \n",
       "max      16.046800    16.159200     7.051500    16.782600    27.485800   \n",
       "\n",
       "           var_195      var_196      var_197      var_198      var_199  \n",
       "count  5557.000000  5557.000000  5557.000000  5557.000000  5557.000000  \n",
       "mean     -0.166249     2.285127     8.918706    15.870934    -3.410438  \n",
       "std       1.421603     5.404760     0.907914     3.029635    10.357434  \n",
       "min      -4.297000   -11.882800     6.322700     7.417100   -36.004500  \n",
       "25%      -1.209700    -1.939200     8.270300    13.869000   -11.259400  \n",
       "50%      -0.193500     2.416300     8.893300    15.908100    -2.979700  \n",
       "75%       0.799800     6.485300     9.601100    18.088800     4.586500  \n",
       "max       3.534500    18.321500    11.722300    24.639400    25.831600  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the taget is really often 0 (about 90%) because the mean is aroun 0.1.\n",
    "\n",
    "Concerning the other variables, there seems not to be a global trend, but they are very numerous.\n",
    "\n",
    "Because they are very numerous, some features could be redundant or irrelevant. Also this could lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X(X, K):\n",
    "    assert K > 0 and isinstance(K, int)\n",
    "    n = len(X)\n",
    "    X_matrix = np.ones((n,1))\n",
    "    for i in range(K):\n",
    "        X_matrix = np.column_stack((X_matrix, X**(i+1)))\n",
    "    return X_matrix\n",
    "\n",
    "def compute_posterior(X, y, sigma2priorweights, sigma2noise):\n",
    "    \n",
    "    Sigma_inverse =    (1/sigma2noise**2) * (X.T.dot(X)) + (1/sigma2priorweights ** 2) * np.eye(X.shape[1])\n",
    "    Sigma = np.linalg.inv(Sigma_inverse)\n",
    "    posterior_mu =     (1/sigma2noise**2) * (Sigma*(X.T).dot(y))\n",
    "    posterior_Sigma =  np.linalg.inv(Sigma)\n",
    "    \n",
    "    return posterior_mu, posterior_Sigma\n",
    "\n",
    "\n",
    "def compute_predictive(Xnew, w_posterior_mu, w_posterior_Sigma, sigma2noise):\n",
    "    print(Xnew.shape)\n",
    "\n",
    "    y_posterior_mu = np.sum(Xnew.dot(w_posterior_mu), axis=1)\n",
    "    y_posterior_sigma2 = (sigma2noise ** 2) + np.sum(Xnew.dot(w_posterior_Sigma).dot(Xnew.T), axis=1)\n",
    "    return y_posterior_mu, y_posterior_sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5558, 201) (5558, 200)\n",
      "(5558, 201) (5558, 200)\n",
      "(5558, 201)\n"
     ]
    }
   ],
   "source": [
    "sigma2noise =  1\n",
    "sigma2priorweights = 1\n",
    "K = 1\n",
    "\n",
    "X_train = build_X(train_data,K)\n",
    "print(X_train.shape,train_data.shape)\n",
    "w_posterior_mu, w_posterior_Sigma = compute_posterior(X_train, train_labels, sigma2priorweights, sigma2noise) \n",
    "\n",
    "X_test = build_X(test_data,K)\n",
    "print(X_test.shape,test_data.shape)\n",
    "y_posterior_mu, y_posterior_sigma2 = compute_predictive(X_test, w_posterior_mu, w_posterior_Sigma, sigma2noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12425095, -0.11474168,  0.03725541, ...,  0.13391458,\n",
       "        0.17417492,         nan])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_posterior_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_posterior_sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.plot(X_train, train_labels, '.k', zorder=100)\n",
    "ax.plot(X_test, y_posterior_mu, '.', color='C1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could drop the features that do not seem correlated to the result by setting a correlation threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to discretize prediction would be to round. If the prediction is higher than 0.5, it is a 1, otherwise, it is a 0.\n",
    "\n",
    "Alternatively, we could then try to move this threshold, because it does not seem to be a lot of prediction above 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class BernoulliLikelihood():\n",
    "    def logdensity(self, y, p):\n",
    "        return np.sum(y * np.log(p) + (1-y) * np.log(1-p))\n",
    "    \n",
    "class NormalPrior():\n",
    "    def __init__(self, sigma2x):\n",
    "        self.sigma2x = sigma2x\n",
    "        \n",
    "    def logdensity(self, x):\n",
    "        return (np.exp(-(x)**2/(2*sigma2x**2))/(sigma2x*np.sqrt(np.pi*2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSampler():\n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples\n",
    "    @samples.getter    \n",
    "    def samples(self):\n",
    "        return np.asarray(self._samples)\n",
    "    \n",
    "    def __init__(self, initial_sample, likelihood, prior):\n",
    "        self.likelihood = likelihood\n",
    "        self.prior = prior\n",
    "        self._samples = [initial_sample]\n",
    "        \n",
    "        \n",
    "    def unnormalized_logposterior(self, w, X, y):\n",
    "        log_likelihood = self.likelihood.logdensity(y, np.sum(logistic(X), axis = 1))\n",
    "        log_prior = np.sum(self.prior.logdensity(w))\n",
    "        return log_likelihood + log_prior\n",
    "\n",
    "    def step(self, X, y, step_proposal):\n",
    "        w_prev = self._samples[-1]\n",
    "        w_proposal = 1 * np.random.randn() + w_prev\n",
    "        \n",
    "        log_gw_prev = self.unnormalized_logposterior(w_prev, X, y)\n",
    "        log_gw_proposal = self.unnormalized_logposterior(w_proposal, X, y)\n",
    "        acceptance_ratio = log_gw_proposal / log_gw_prev\n",
    "        \n",
    "        \n",
    "        if acceptance_ratio >= 1:\n",
    "            self._samples.append(w_proposal)\n",
    "        else:\n",
    "            u = np.random.uniform(0, 1) \n",
    "            if u <= acceptance_ratio:\n",
    "                self._samples.append(w_proposal)\n",
    "            else:\n",
    "                self._samples.append(w_prev)\n",
    "        \n",
    "        return min(acceptance_ratio, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2x = 1\n",
    "\n",
    "likelihood = BernoulliLikelihood()\n",
    "prior = NormalPrior(sigma2x)\n",
    "\n",
    "starting_point = np.sum(logistic(X_train))/X_train.shape[0]\n",
    "sampler = MHSampler(starting_point, likelihood, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in log\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c6245614291a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-e4f408960946>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, X, y, step_proposal)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mw_proposal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlog_gw_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnormalized_logposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlog_gw_proposal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnormalized_logposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_proposal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0macceptance_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_gw_proposal\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlog_gw_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-e4f408960946>\u001b[0m in \u001b[0;36munnormalized_logposterior\u001b[0;34m(self, w, X, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munnormalized_logposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-8018c01a83e7>\u001b[0m in \u001b[0;36mlogistic\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBernoulliLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogdensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(10000):\n",
    "    sampler.step(X_train,train_labels,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_new, w_samples):\n",
    "    return np.sum(logistic(np.array([w_samples]).T.dot(np.array([x_new]))))\n",
    "\n",
    "predict(X_test, sampler.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am sorry but my code from the lab seems to be wrong (even if I thought that it were good because it gave coherent results in the lab).\n",
    "\n",
    "I no longer have time to make it right, therefore I am not able to continue. But I will try, still, to analyse and comment\n",
    "\n",
    "\n",
    "We assumed a gaussian prior, but we could use Laplace approximation to make sure that we will obtain a posterior which follow the same distribution.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
